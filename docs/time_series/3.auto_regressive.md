# OLS for Time Series Analysis
From previous lessons, we know that often observations in a time series are correlated with each other. If we want to use the past observations to predict today's observations, the first thing that comes to mind is to use OLS (ordinary least square regression). In regular OLS, we have a dependent variable $Y$ and an independent variable $X$. We can write a linear regression as follows:

$$ Y = C+b_{1} X_{1} + \varepsilon $$ 

where $C$ is the intercept and $\varepsilon$ is the error term.

To apply OLS to a time series $X_t$ as a dependent variable, and its lag 1, $X_{t-1}$ as an independent variable, we can write the OLS as follows:

$$ X_{t} = C+b_{1}X_{t-1}+\varepsilon_{t} $$ 

where $C$ is the intercept and $\varepsilon_{t}$ is the normal white noise and absolute value of $b_{1} < 1$. 

We then can apply OLS to estimate the coefficients and the residuals and to conduct model diagnostics. The above regression model for time series $X_t$ is called an **autoregressive (AR) model**.

## **2. Autoregressive (AR) Model**
An autoregressive model is a regression model to predict a time series using a linear combination of its pass values as independent variables, like what we demonstrated in the last section for time series $X_t$. We set up a regression model to predict $X_t$ using its lag 1 variable $X_{t-1}$ as the independent variable. Because there is only one lag 1 variable on the right side of the equation, we also denote this autoregressive model as AR(1). 

We just described the concept of an autoregressive model. Now let's formally define an autoregressive model.

The formula for an autoregressive model with the order of $p$, denoted as AR($p$), is as follows:

$$ X_{t} = \alpha_{1} X_{t-1} + \alpha_{2} X_{t-2} + \cdots + \alpha_{p} X_{t-p} + W_{t} $$ 

Where $X_{t}$ is stationary, $\alpha_{1}, \alpha_{2}, \cdots, \alpha_{p}$ are constants and $W_{t}$ is normal white noise.  

If we use backshift operator $B$, we can rewrite AR($p$) as follows:

$$ \alpha(B) X_{t} = (1 - \alpha_{1} B - \alpha_{2} B^{2} - \cdots - \alpha_{p} B^{p}) X_{t} = W_{t} $$

$\alpha(B) = (1 - \alpha_{1} B - \alpha_{2} B^{2} - \cdots - \alpha_{p} B^{p})$ is defined as an autoregressive operator.

There are restrictions for the value of coefficients $\alpha$ to make sure $X_{t}$ can stay stationary.

> For AR(1), $-1 < \alpha_{1} < 1$.

>For AR(2), 
$ \begin{cases}
  -2 < \alpha_{1} < 2, \\
  -1 < \alpha_{2} < 1, \\
  \alpha_{1} + \alpha_{2} < 1, \\
  \alpha_{2} - \alpha_{1} < 1
\end{cases}$.

For a higher degree of AR, the requirement is as follows:

> 1. Set autoregressive operator = $0$ as follows: $ (1 - \alpha_{1} B - \alpha_{2} B^{2} - \cdots - \alpha_{p} B^{p}) = 0 $. This equation is called a characteristic equation.
2. Solve the characteristic equation for $B$ and get all the roots of $B$.
3. The absolute values of all the roots of $B$ need to be $> 1$ to ensure $X_{t}$ is stationary.

We will talk more about these requirements later in this course.

We can see from the AR definition above that random walk is very similar to AR(1) model except that $|\alpha_{1}|=1$. Hence, random walk is not stationary.
<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>

AR(1) model can be written as follows:

$$ X_{t} = C + \alpha_{t} X_{t-1} + W_{t} $$

Where $C$ is a constant and $W_{t}$ is normal white noise with mean = $0$ and variance = $\sigma^{2}$

Now let's look at AR(1)'s mean, variance, autocovariance, ACF, and PACF.

The **mean** function is:

$$ \mu = \frac{C}{1-\alpha_{1}} $$

When there is no intercept $C$ in the process, $\mu$ is $0$.

The **variance** is:

$$ \frac{\sigma^{2}}{1-\alpha^{2}} $$

**Autocovariance** for $X_{t}$ and $ X_{s}$ is:

$$ \gamma(h) = \Big(\frac{\sigma^{2}}{1-\alpha^{2}} \Big) \alpha^{|h|}, \ \text{ where } |h| = |s – t| $$

**Autocorrelation** for $X_{t}$ and $ X_{s}$ is:

$$ \rho(h) =(\alpha^{|h|}), \ \text{ where } |h| = |s – t| $$

**Partial Autocorrelation** for $X_{t}$ and $ X_{s}$ is:

$$ \phi_{hh}= \begin{cases}
  \alpha  &  \text{ for } |h| \le 1 \\
  0  &  \text{ for } |h| > 1
\end{cases} $$


One interesting feature about AR process or AR model is that we can represent an MA process using an infinite AR process. Let's use MA(1) as an example.

We have the following MA(1) process:

$$ X_{t} = W_{t} + \theta W_{t-1} $$ 

where $W$ is the normal independent white noise with mean = $0$ and variance = $1$. If and only if $| \theta | < 1$, we can write $X_{t}$ in an infinite AR form as follows:

$$ X_{t} = \theta X_{t-1} - \theta^{2} X_{t-2} + \theta^{3} X_{t-3} -\cdots - W_{t} $$

Here is how to derive the above result:

$$ X_{t} = W_{t} + \theta W_{t-1} $$ 

$X_{t} = (1 + \theta B) W_{t}$  by using the backshift operator

$W_{t} = \frac{1}{1+\theta B}X_{t}$  by rearranging the terms

We also know the formula of infinite geometric series:

$$ S_{\infty } = a + ar + ar^{2} + ar^{3} + \cdots + ar^{\infty } = \frac{a}{1-r} $$ when $|r| < 1$.

If $| \theta | < 1$, we can rewrite $\frac{1}{1+\theta B}X_{t}$ as follows:

$$ \frac{X_{t}}{1+\theta B} = X_{t} + X_{t}(-\theta B) + X_{t}(-\theta B)^{2} + X_{t}(-\theta B)^{3} + \cdots + X_{t}(-\theta B)^{\infty} $$

$$ = X_{t} - \theta B X_{t} + \theta^{2} B^{2} X_{t} - \theta^{3} B^{3} X_{t} + \cdots + \theta^{\infty } B^{\infty } X_{t} $$

We then can plug this back into $W_{t} = \frac{1}{1+\theta B}X_{t}$ and rearrange the terms and we will get:

$$ X_{t} = \theta B X_{t} - \theta^{2} B^{2} X_{t} + \theta^{3} B^{3} X_{t}+ \cdots + W_{t} $$

Then, we can place the backshift operator back as the subscript for $X_{t}$ and we get:

$$ X_{t} = \theta X_{t-1} - \theta^{2}X_{t-2} + \theta^{3}X_{t-3} + \cdots + W_{t} $$

Now we know that $X_{t}$ can be written as an infinite AR model from an MA(1) model.

The key point for MA(1) to be able to be represented as an infinite AR process is to make sure its coefficient $| \theta | < 1$. We call this MA(1) **invertible** if the MA(1) process has the absolute value of coefficients less than 1 and can be represented by an infinite AR process. 

The other benefit when an MA process is invertible is that the MA process is also **unique**. Let's see an example below to demonstrate this concept.

> $\text{MA}_A$: $X_{t} = W_{t} + 5W_{t-1}$ with $W \sim \text{normal white noise}(0,1)$
>
> $\text{MA}_B$: $X_{t} = W_{t} + \frac{1}{5}W_{t-1}$ with $W \sim \text{normal white noise}(0,25)$


Both $\text{MA}_A$ and $\text{MA}_B$ time series have the same autocovariance values shown as follows:

$$ \gamma(h)= \begin{cases}
  26  &  \text{ for }  h=0 \\
  5  &  \text{ for }  h=1 \\
  0  &  \text{ for }  h \gt 1
\end{cases} $$

In the real world, we only observe the autocovariance values of the time series, not the actual model. We cannot tell if $\text{MA}_A$ or $\text{MA}_B$ is the correct one. Both $\text{MA}_A$ and $\text{MA}_B$ are the model candidates for the underlying time series data. So which model should we choose? If we need the MA model to be invertible, then we will limit our model choice to $\text{MA}_B$, which has the absolute value of coefficient less than $1$. Then, we call the $\text{MA}_B$ model unique. 

### **2.6 Estimation of AR Model**
We have introduced what an AR process model is. We discussed its property and how to select the number of lags $p$ for an AR($p$) model. Now we need to estimate the coefficients of the model. There are a few methods to estimate the model. We saw in section 1 that we can use ordinary least square (OLS) method to estimate the model. In this section, we will go over two other methods here: **method of moments** and **maximum likelihood method**.

#### **2.6.1. Method of Moments (MOM)**
Before talking about method of moments, let's briefly review what **moments** are. Moments are statistical parameters used to describe the distribution of a random variable. 

The most popular ones are mean and variance. Mean is the first order moment. Variance and covariance are the second order moments. Why do we refer to these moment parameters with order? It is because the functions of these parameters include the expected values of the order of random variable($E(X^{n})$). For example, the mean of $X$ is $E(X)$. The variance of $X$ can be written as $E(X^{2})-E(X)^{2}$. Hence, mean is the first order moment and variance is the second order moment for a random variable. Here is the list of popular moments we have used so far:

> - First Order Moment: Mean
> - Second Order Moments: Variance, covariance, correlation, autocovariance, autocorrelation
> - Third Order Moment: Skewness
> - Fourth Order Moment: Kurtosis

Now we know what a moment is. Next, we are going to explain what **method of moments** is. Method of moments is an estimation method that equates a sample moment to its theoretical counterpart and allows us to estimate the parameters for the model. Let's use a very simple example in AR(1) model to explain how it works.

Assume we have a time series $X_{t}$ and we have removed the trend and the seasonality. The time series is stationary. We have also run ACF and PACF and have decided AR(1) is a good model specification. We assume the mean of $X_{t}$ is $0$ so there is no constant in the AR(1) model. We can write our AR(1) model as follows:

$$ X_{t} = \alpha_{t} X_{t-1} + W_{t} $$

Where $W_{t}$ is normal white noise with mean = $0$ and variance = $\sigma^{2}$


From the time series $X_{t}$, we can calculate the following information:

> - Sample mean: $\overline{X}$
> - Sample variance: $\widehat{\gamma(0)}$
> - Sample autocovariance of lag 1: $\widehat{\gamma(1)}$


From last section, we know variance and autocovariance formulas are as follows:

The variance is:

$$ \frac{\sigma^{2}}{1-\alpha^{2}} $$

Autocovariance for $X_{t}$ and $ X_{s}$ is:

$$ \gamma(h) = (\frac{\sigma^{2}}{1-\alpha^{2}}) \alpha^{|h|} $$

where $|h| = |s – t| $

We can use the above sample information and formulas to solve for AR(1) parameter $\alpha$.

$$ \widehat{\alpha} = \frac{\widehat{\gamma(1)}}{\widehat{\gamma(0)}} $$

#### **2.6.2 Yule-Walker Estimators for AR Model**
We introduced MOM in the last section to estimate an AR(1) model. We can generalize the method we used in the last section to solve for a higher order AR($p$) model. The method we used above is a very simple case of Yule-Walker estimation. The **Yule-Walker estimation method** connects the coefficients of an AR($p$) model to the autocovariances of the time series by using a system of equations called **Yule-Walker equations**. 

When we have an AR($p$) process as follows:

$$ X_{t} = \alpha_{1} X_{t-1} + \alpha_{2} X_{t-2} + \cdots + \alpha_{p} X_{t-p} + W_{t} $$

Yule-Walker equations can be defined as follows:

$$ \begin{align*}
  \gamma(1)  &=  \alpha_{1}\gamma(0)+\alpha_{2}\gamma(-1)+\alpha_{3}\gamma(-2)+\cdots +\alpha_{p}\gamma(1-p) \\
  \gamma(2)  &=  \alpha_{1}\gamma(1)+\alpha_{2}\gamma(0)+\alpha_{3}\gamma(-1)+\cdots +\alpha_{p}\gamma(1-p+1) \\
  \gamma(3)  &=  \alpha_{1}\gamma(2)+\alpha_{2}\gamma(1)+\alpha_{3}\gamma(0)+\cdots +\alpha_{p}\gamma(1-p+2) \\
  &\ \ \vdots \\
  \gamma(p)  &=  \alpha_{1}\gamma(p-1)+\alpha_{2}\gamma(p-2)+\alpha_{3}\gamma(p-3)+\cdots+\alpha_{p}\gamma(0) \\
  \ \sigma^{2} \ \ &=  \gamma(0)-\alpha_{1}\gamma(1)-\alpha_{2}\gamma(2)-\cdots-\alpha_{p}\gamma(p)
\end{align*} $$

With the above $p+1$ Yule-Walker equation system, we can solve for $\{ \alpha_{1}, \alpha_{2}, \alpha_{3}, \cdots, \alpha_{p}, \sigma^{2} \}$ $\ p+1$ parameters. We can see the last equation in the Yule-Walker equation system is an equation to link the white noise variance to model coefficients and autocovariance. 

All statistical applications can calculate the estimated coefficients using Yule-Walker equations, so you do not need to calculate them manually.
#### **2.6.3 Maximum Likelihood Method (Box-Jenkins Method)**
The next estimation method we will introduce is the **maximum likelihood method (MLE)**. The maximum likelihood method is used to maximize the joint probability of the observed data points with respect to the model parameters. The joint probability is called a **likelihood function**. The likelihood function will usually take the log form so that the function will be easier to handle. Please read the required reading to learn the general definition and properties of the maximum likelihood estimation method. 

The maximum likelihood estimation method is the selected estimation method in the Box-Jenkins method. We will talk more about the Box-Jenkins method in the next lesson.

To apply the maximum likelihood method to an AR(1) model, we assume we have the following AR(1) model:

$$ X_{t} = \alpha_{t} X_{t-1} + W_{t} $$

Where $W_{t}$ is normal white noise with mean = $0$ and variance = $\sigma^{2}$ and $|\alpha| < 1$

With the above model, we can write our likelihood function as follows:

$$ L (\alpha,\sigma^{2}) = f(x_{1}, x_{2}, x_{3}, \cdots, x_{n} | \alpha, \sigma^{2}) $$

Where $x_{1}, x_{2}, x_{3}, \cdots, x_{n}$ are data points from the time series.

We then maximize $L(\alpha,\sigma^{2})$ with respect to $\alpha$ and $\sigma^{2}$ to solve for $\alpha$. All the statistical applications can solve the optimization for us.

#### **2.6.3 Maximum Likelihood Method (Box-Jenkins Method)**
The next estimation method we will introduce is the **maximum likelihood method (MLE)**. The maximum likelihood method is used to maximize the joint probability of the observed data points with respect to the model parameters. The joint probability is called a **likelihood function**. The likelihood function will usually take the log form so that the function will be easier to handle. Please read the required reading to learn the general definition and properties of the maximum likelihood estimation method. 

The maximum likelihood estimation method is the selected estimation method in the Box-Jenkins method. We will talk more about the Box-Jenkins method in the next lesson.

To apply the maximum likelihood method to an AR(1) model, we assume we have the following AR(1) model:

$$ X_{t} = \alpha_{t} X_{t-1} + W_{t} $$

Where $W_{t}$ is normal white noise with mean = $0$ and variance = $\sigma^{2}$ and $|\alpha| < 1$

With the above model, we can write our likelihood function as follows:

$$ L (\alpha,\sigma^{2}) = f(x_{1}, x_{2}, x_{3}, \cdots, x_{n} | \alpha, \sigma^{2}) $$

Where $x_{1}, x_{2}, x_{3}, \cdots, x_{n}$ are data points from the time series.

We then maximize $L(\alpha,\sigma^{2})$ with respect to $\alpha$ and $\sigma^{2}$ to solve for $\alpha$. All the statistical applications can solve the optimization for us.
