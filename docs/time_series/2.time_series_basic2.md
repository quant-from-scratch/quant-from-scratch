## **1. Remove Trend from Non-Stationary Time Series**
In the last lesson, we saw that one of the key points of time series analysis is to understand the relationship of data observations in different time points for a time series. We introduced ACF and PACF that are key metrics to understanding the relationship among the observations in a time series. We also introduced stationary time series. A stationary time series means that its mean and autocovariance are independent of absolute time location of data observations. For example, $Cov(X_t, X_s)$ must depend only on the difference $t-s$. The estimation of ACF and PACF will be accurate and stable without the worry of which section of data we choose to estimate. Hence, it is critical to obtain a stationary time series before doing modeling work.

Removing trends from a non-stationary time series is usually the first step to handling the time series data. In this section, we will go through two common methods to remove a trend: using a trend variable or differencing. We will use Google stock as an example to illustrate the process.


### **1.1 Remove Trend with a Trend Variable**

# Detrend
timeTrend = np.linspace(1, len(goog.GOOGLE), len(goog.GOOGLE))
timeTrend = sm.add_constant(timeTrend)

# Fit OLS
model = sm.OLS(goog.GOOGLE, timeTrend)
fit_g = model.fit()
fit_g.summary()

# Plot residuals
goog_res = fit_g.resid
goog_res.plot(linewidth=1.3, xlabel="Year", ylabel="Residuals")
plt.show()

From figure 3, we can see the residuals from the simple regression model with a trend variable does not appear to exhibit stationarity. The time series of residuals shows what resembles a "V pattern," and therefore, it does not appear that the mean is constant over time. Thus, the residuals from the regression do not form a stationary time series. 

Adding a trend variable does not seem to be enough to make Google stock price into a stationary time series.

Let's look at the differencing method.


### **2.2 Removing Trend Using the Differencing Method**
#### **2.2.1 Notations**
The first one we will introduce is first difference. First difference is defined as:

$$ \nabla x_{t} = x_{t} - x_{t-1} $$

And second difference can be written as:

$$ \nabla^{2} x_{t} = \nabla (\nabla x_t) = \nabla(x_{t}-x_{t-1}) = (x_{t}-x_{t-1}) - (x_{t-1}-x_{t-2}) $$

The next notation is backshift operator. The backshift operator is defined as:

$$ B x_{t} = x_{t-1} $$

And

$$ B^{2} x_{t} = B (B x_{t}) = B x_{t-1} = x_{t-2} $$

From above, we can write a general backshift operator as follows:

$$ B^{k} x_{t} = x_{t-k} $$

With the backshift operator, we can also write the first difference with backshift operator:

$$ \nabla x_{t} = x_{t}-x_{t-1} = x_{t}-Bx_{t} = (1-B)x_{t} $$

Then, the second difference can be defined and written as follows:

$$ \nabla ^{2} x_{t} = (1-B)^{2} x_{t} = (1-2B+B^{2}) x_{t} = x_{t} - 2 x_{t-1} + x_{t-2} $$

And the difference of order $d$ can be defined as:

$$ \nabla^{d} = (1-B)^{d} $$

The differencing method is used to generate first difference time series. Let's keep using Google stock price as an example. Figure 4 shows the first difference of Google stock price.

## **3. White Noise**
**White noise** is a special time series model. The white noise time series is generated from a collection of uncorrelated random variables. We can define white noise time series ${W_{t}}$ as follows:

$$ {W_{t}}\sim WN(0,\sigma^{2}) $$

Its autocovariance is given by:

$$ \gamma_{_{W}}(h) = \begin{cases}
  \sigma_{2}  & \text{ for }  h=0 \\
  0  &  \text{ for } |h| \gt 0
\end{cases} $$

Since both mean and autocovariance of white noise are independent of $t$, white noise is a stationary time series.

If the white noise random variables are also independent and identical, we can denote it as follows:

$$ {W_{t}} \sim iid(0,\sigma^{2}) $$

If all the random variables are i.i.d. and normally distributed, this is a normal white noise time series and we can denote it as follows:

$$ {W_{t}} \sim iid N(0,\sigma^{2}) $$

Let's produce one simulation of a white noise process (see figure 6).
<span style='color: transparent; font-size:1%'>All rights reserved WQU WorldQuant University QQQQ</span>


# Normal white noise ts example
plt.plot(np.random.normal(0, 3, 300))
plt.title("Normal White Noise with Mean=0 and Variance=9")
plt.show()

Figure 6 shows a white noise time series with mean = $0$ and variance = $9$. We can see the time series is pretty choppy, but it oscillates around $0$. 

Normal independent white noise is similar to the normal independent error terms in OLS regression. Usually, a time series can be broken down into a trend component, a seasonality component, and a residual component. If we can model the residual using a white noise model, then we will be able to model the whole time series and predict it. The random nature of the white noise makes it a good choice as a random term in the following time series models. 

## **4. Random Walk**
The next time series model we would like to talk about is the random walk time series. A random walk time series says that the current value depends only on the past value with a random component. We can write down the math definition as follows:

If $\{ X_{t} \}$ is a random walk time series and $x_0 = 0$, then $X_{t} = X_{t-1} + W_{t}$ where ${W_{t}} \sim WN(0,\sigma^{2})$

From the above definition, you can replace $X_{t-1}$ with $X_{t-2} + W_{t-1}$ and continue the same replacement process until $X$ is represented with $W_{i}$ where $i = 1, \cdots, t$ and $ X_{0}$. We call this a back substitution technique. After you do the process, you can replace the above definition with the following formula:

$$ X_{t} = \sum_{i=1}^{t} W_{i} + X_{0} = \sum_{i=1}^{t} W_{i} $$

The mean function:

$$  E(X_{t}) = E(X_{t-1} + W_{t}) = E(\sum_{i=1}^{t} W_{i} + X_{0}) = X_{0}=0 $$

From the mean function, we know it is not dependent on $t$ but on the first time $X_{0}$. In this case, it is $0$.

The autocovariance:

$$ COV(X_{t+h},X_{t}) = COV(X_{t} + W_{t+1} + W_{t+2} + \cdots + W_{t+h}, X_{t}) = Cov(X_{t},X_{t}) = t \sigma^{2} $$

We can see the autocovariance is dependent on $t$. Hence, the random walk time series is not a stationary time series.

In figure 7, we simulated four random walk time series of 300 data points with the same setup: normal white noise and $X_{0}=0$. Every time we generate a time series from the same random walk model, we'll get a very different random walk time series. There is no clear common pattern. We can also see random walk is not confined in a range as a stationary time series is. 

One common random walk time series variation is **random walk with a drift**. Random walk with a drift is to add a constant term $D$ to a random walk model. We can write random walk with a drift model as follows:

If $\{ X_t \}$ is a random walk with a drift time series and $x_{0} = 0$, then $X_{t} = D + X_{t-1} + W_{t}$ where ${W_{t}} \sim WN(0,\sigma^{2})$ and $D$ is a constant

We call $D$ a drift. With the back substitution technique, we can rewrite the model as follows:

$$ X_{t} = D t + \sum_{i=1}^{t} W_{i}+X_{0} = D t +\sum_{i=1}^{t} W_{i} $$

The mean function:

$$ E(X_{t}) = D t + \sum_{i=1}^{t} E(W_{i}) = D t $$

From the mean function, we see it is dependent on t.

The autocovariance:

$$ COV(X_{t+h},X_{t}) = COV(X_{t} + W_{t+1} + W_{t+2} + \cdots + W_{t+h}, X_{t}) = Cov(X_{t},X_{t}) = t \sigma^{2} $$

As the difference of random walk and random walk with a drift is only the addition of a constant drift, their autocovariance should be the same. We can see that random walk with a drift is not a stationary time series either. Both mean and autocovariance depend on $t$. Also because of the addition of drift, the mean is increasing as time increases. Hence, random walk with a drift is a choice to model real world time series that possess a trending pattern. 


## **5. Moving Average**
A moving average model tries to model a shock with impact in a finite time frame. If a shock in interest has an initial impact and the impact gradually fades in a finite time frame, the moving average model can be a model choice. A moving average with a shock lasting only for a total time range of $q$ can be written as MA($q$) and is defined as follows:

$$ X_{t} = W_{t} + \theta_{1} W_{t-1} + \theta_{2} W_{t-2} + \cdots + \theta_{q} W_{t-q} $$

Where $\theta_{1}, \theta_{2}, \cdots, \theta_{q}$ are parameters and $W_{t}$ is normally distributed white noise with mean = $0$ and variance $\sigma^{2}$ is $1$. There are $q$ lags in the model.

We can also write the MA($q$) model using backshift operator:

$$ X_{t} = \theta(B) W_{t} $$

Where $\theta(B)$ is the **moving average operator** as follows:

$$ \theta(B) = 1 + \theta_{1} B + \theta_{2} B^{2} + \cdots + \theta_{q} B^{q} $$

The following plots show some simulated examples for moving average models.

From figure 9, we can see the time series in the bottom plot is more volatile than the time series in the top plot. Now, let's check the mean, autocovariance, and autocorrelation of MA(1) process.

Mean: 

$$ E(X_{t}) = E(W_{t} + \theta W_{t-1})=0 $$

Autocovariance:

$$ \gamma(h)= \begin{cases}
  (1+\theta^{2}) \sigma^{2}  & \text{ for }  h=0 \\
  \theta \sigma^{2}  &  \text{ for }  h=1 \\
  0  &  \text{ for }  h\gt 1 
\end{cases} $$

ACF:

$$ \rho(h)= \begin{cases}
  \frac{\theta}{(1+\theta^{2})}  &  \text{ for } h=1 \\
  0  &  \text{ for }  h \gt 1 
\end{cases} $$

From the above formula, we know the mean and autocovariance for MA time series are independent of time. Hence, MA time series are stationary. Let's look at the ACF plots for MA(1).

## **6. Conclusion**
In this lesson, we first discussed two methods to remove a trend in a time series. The first method is to introduce a trend variable to a simple regression model. The second method is to use first difference. We then started to go through some time series statistical models. We talked about white noise as an error term for time series. Then, we talked about random walk and random walk with a drift to model non-stationary time series. We then introduced moving average model and applied it to Google stock price. In the next lesson, we will continue our journey and learn more time series statistical models.
