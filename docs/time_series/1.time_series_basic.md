# Time series characteristics
From the last three figures we have noticed several patterns:

> a. trend <br>
> b. cyclical movement (seasonality) <br>
> c. volatility clusters (volatilities can be different during different time periods) <br>
> d. correlation between observations <br>
> e. extreme values or outliers <br>

# Time series Definition
The data series $ \{ x_1, x_2,\cdots ,x_{t-1}, x_t \}$ is called a time series if 

> a. $t$ is an ordered time stamp (date, hour, year, etc.) <br>
> b. $x_{t}$ is from a random variable $X_{t}$ <br>

**Figure 4: Time Series Definition Example**

| Date | Price: <br>Observed Data | Associated <br>Random Variable |
| :---: | :---:   | :---:   |
| day 1 | $$x_1$$ | $$X_1$$ |
| day 2 | $$x_2$$ | $$X_2$$ |
| day 3 | $$x_3$$ | $$X_3$$ |
| day 4 | $$x_4$$ | $$X_4$$ |
| day 5 | $$x_5$$ | $$X_5$$ |

**Figure 5: Lag Expressions for Time Series**

| Time Series   | | $$\hspace{1.5cm}$$ | $$\hspace{1.5cm}$$ | Data Points | $$\hspace{1.5cm}$$ | $$\hspace{1.5cm}$$ |
| :---:             | |  :---:  |  :---:  |  :---:  |  :---:  |  :---:  |
| $$X_5$$           | | $$x_5$$ | $$x_4$$ | $$x_3$$ | $$x_2$$ | $$x_1$$ |
| $$X_{(5-1)}=X_4$$ | | $$x_4$$ | $$x_3$$ | $$x_2$$ | $$x_1$$ |         |
| $$X_{(5-2)}=X_3$$ | | $$x_3$$ | $$x_2$$ | $$x_1$$ |         |         |


**Figure 6: Time Series Difference Calculation Example**

| Time Series <br>Difference Calculation   | | $$\hspace{1.5cm}$$ | $$\hspace{1.5cm}$$ | Data Points | $$\hspace{1.5cm}$$ | $$\hspace{1.5cm}$$ |
| :---:             | |  :---:  |  :---:  |  :---:  |  :---:  |  :---:  |
| $$X_5-X_4$$ | | $$x_5-x_4$$ | $$x_4-x_3$$ | $$x_3-x_2$$ | $$x_2-x_1$$ |         |
| $$X_5-X_3$$ | | $$x_5-x_3$$ | $$x_4-x_2$$ | $$x_3-x_1$$ |         |         |


# Autocorrelation
We discussed how one of the assumptions for OLS regression is that each observation should be independent of each other. If the data violates this assumption, this data series has **autocorrelation**. In our last section, we saw U.S. 10-Year Treasury bond yields have an autocorrelation issue because today's bond yield is likely to go down if the yields over the past few days have been inclining. The issue of correlated observations is one challenge in analyzing time series data. We cannot apply time series data to OLS properly without addressing the issue of autocorrelation. So how do we analyze the autocorrelation for a time series? We will introduce the autocorrelation function as a measure in this section. Before getting into autocorrelation function, we would like to introduce some concepts here.

## Mean function of time series
Great questions! Let’s break down the **mean function for time series** and see why it’s important.

### 1. **What is the Mean Function?**

The **mean function** for a time series, denoted \( \mu_{X_t} \), represents the *expected value* (or average) of the random variable \( X_t \) at a specific time \( t \). In simpler terms, it’s a way to describe the "center" or "average" value of the time series at that time.

### 2. **Why Do We Need the Mean Function?**

In time series analysis, understanding the "center" of the data helps in:
- **Identifying Trends**: The mean can help us see if the series is increasing, decreasing, or staying constant over time.
- **Modeling and Prediction**: Many time series models are based on understanding the mean because it helps in forecasting future values.
- **Removing Noise**: The mean serves as a reference point. Deviation from the mean (like variance) tells us how much the values vary over time.

### 3. **How Do We Come Up with This Formula?**

The formula:
$$ \mu_{X_t} = E(X_t) = \int_{a}^{b} x f_t(x) \, dx $$
is based on the **definition of the expected value** of a continuous random variable.

Here’s a breakdown:
- **Expected Value \( E(X_t) \)**: This is the theoretical average or mean of \( X_t \), which we calculate based on its probability distribution.
- **Integral \( \int_{a}^{b} x f_t(x) \, dx \)**: This integral is how we calculate the expected value for a continuous variable. It "adds up" all possible values \( x \), weighted by their probabilities, over the interval \([a, b]\).
- **\( f_t(x) \)**: This is the probability density function (PDF) of \( X_t \), which tells us the likelihood of different outcomes for \( X_t \) at time \( t \).

In summary:
- The mean function is crucial for describing the center of a time series.
- It helps in analyzing trends, building models, and removing noise.
- The formula is derived from the definition of the expected value for continuous random variables.

## Autocovariance function
The first metric we can use to study the strength of the relationship between two observations in a time series is **autocovariance function**. Autocovariance function measures the linear dependence of two observations. Autocovariance function plays a similar role to covariance for two data series, except that autocovariance function assesses the relationship among observations in one data series.

Assume $X_t$ has finite variance where $t = 1, 2, \cdots, T$, autocovariance function is defined as:

$$ \gamma_{_{X}}(s,t) = cov(X_{s}, X_{t}) = E[ (X_{s}-\mu_{_{X_{s}}})(X_{t}-\mu_{_{X_{t}}}) ] \ $$  

for all $s$ and $t$



Here are some properties for autocovariance function:

> a. It only measures the linear relationship between $X_{s}$ and $X_{t}$ <br>
> b. $\gamma_{_{X}}(s,t) = \gamma_{_{X}}(t,s) $ <br>
> c. When $\gamma_{_{X}}(s,t)$ is close to $0$ and the distance between $s$ and $t$ is large, the time series graph would exhibit a choppy curve between $s$ and $t$ <br>
> d. When the absolute value of $\gamma_{_{X}}(s,t)$ is large and the distance between $s$ and $t$ is large, the time series graph would exhibit a smooth curve between $s$ and $t$ <br>
> e. When $s = t$, the autocovariance becomes variance of $X_t$ <br>

## Autocorrelation  (ACF)
Now we can introduce the autocorrelation function (ACF). Its definition is:

$$ \rho_{_{X}}(s,t) = \frac{\gamma_{_{X}}(s,t)}{\sqrt{\gamma_{_{X}}(s,s)\gamma_{_{X}}(t,t)}} $$

Here are some properties for ACF:

> a. It only measures the linear relationship between $X_{s}$ and $X_{t}$ <br>
> b. The value of $\rho_{_{X}}(s,t) $ is between $-1$ and $1$ <br>

By using autocorrelation, we can see if the current observation has any significant correlation with observations in different time lags.

## Partial autocorrelation 
In the previous section, we talked about using autocorrelation to evaluate the relationship of two observations in a time series. Say if we have $x_{t}$ and $x_{t-3}$ from a time series, we can write their autocorrelation as follows:

$$ \rho_{_{X}}(t,t-3) = \frac{cov(x_t, x_{t-3})}{\sqrt{var(x_t) var(x_{t-3})}} $$

The above autocorrelation is to assess the relationship between $x_{t}$ and $x_{t-3}$ including the correlation explained by $x_{t-1}$ and $x_{t-2}$. However, let's assume we want to evaluate the pure correlation between $x_{t}$ and $x_{t-3}$ stripped out correlation from $x_{t-1}$ and $x_{t-2}$. In this case, we would need to use partial autocorrelation (PACF). Partial autocorrelation can be written as follows:

$$ \phi_{_{X}}(t,t-3 \ | \ t-1,t-2) = \frac{cov(x_t,x_{t-3} | x_{t-1},x_{t-2})}{\sqrt{var(x_t | x_{t-1},x_{t-2}) var(x_{t-3} | x_{t-1},x_{t-2})}} $$

As you can see, partial autocorrelation is a conditional correlation. Unlike autocorrelation, partial correlation is giving the information about the pure correlation between $x_{t}$ and $x_{t-3}$ that $x_{t-1}$ and $x_{t-2}$ do not explain. For PACF notation, we can drop the condition part and simply write the notation as follows:

$$ \phi_{_{X}}(t,t-3) $$

Both ACF and PACF are very useful when we talk about time series regression analysis later.

# Stationary
Sure! Here’s a simplified and more concise explanation:

---

### Stationarity in Time Series

In time series analysis, **stationarity** refers to a property that makes a time series easier to model and analyze.

A time series \( \{ X_t \} \) with finite variance is **stationary** if:

1. **Constant Mean**: The mean \( \mu_X(t) = E(X_t) \) is the same at all times \( t \).
   
2. **Constant Autocovariance**: The autocovariance between two points in time depends only on the time difference \( h \), not on the actual time \( t \). For example, \( \text{Cov}(X_t, X_{t+h}) = \text{Cov}(X_s, X_{s+h}) \).

This implies that the **variance is also constant over time** (no heteroskedasticity), and this definition is called **weak stationarity**.

---

### Strict Stationarity

For a time series \( \{ X_t \} \) to be **strictly stationary**, the joint probability distribution of any collection of points \( \{ x_{t_1}, x_{t_2}, \dots, x_{t_k} \} \) must be identical to the distribution of the same points shifted in time \( \{ x_{t_1+h}, x_{t_2+h}, \dots, x_{t_k+h} \} \). 

This can be expressed as:
\[
P \{ x_{t_1} \le c_1, \dots, x_{t_k} \le c_k \} = P \{ x_{t_1+h} \le c_1, \dots, x_{t_k+h} \le c_k \}
\]
for any time shift \( h \) and any values \( c_1, c_2, \dots, c_k \).

Strict stationarity is a stronger requirement than weak stationarity, and most statistical methods in time series only require weak stationarity.

---

### Notation in Stationary Time Series

In a stationary series, we can simplify notation:
- The **mean** \( \mu_t \) becomes \( \mu \), a constant.
- The **autocovariance** depends only on \( h \), so we write it as \( \gamma_X(h) \).
- **ACF** (Autocorrelation Function) and **PACF** (Partial Autocorrelation Function) also depend only on \( h \), and we write them as \( \rho_X(h) \) and \( \phi_X(h) \).

---

### Why Stationarity Matters

Many time series models require a stationary series to work effectively. Ensuring stationarity (often weak stationarity) allows us to make better predictions and analyze time-dependent data more reliably. Later in the course, we’ll discuss ways to test for stationarity before applying time series models.

